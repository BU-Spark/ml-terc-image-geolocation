{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS, GPSTAGS\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from scipy.signal import correlate2d, find_peaks\n",
    "import cv2\n",
    "from openpyxl import load_workbook\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exif_data(image_path):\n",
    "    \"\"\"\n",
    "    Function to extract specific EXIF metadata - GPS coordinates, \n",
    "    camera model, and focal length from an image file. \n",
    "    --------------------------------------------------------------\n",
    "    Parameters:\n",
    "    image_path: A string representing the file path of the image from which the EXIF data is to be extracted.\n",
    "    --------------------------------------------------------------\n",
    "    Returns:\n",
    "    A tuple of lists containing the EXIF tags and their corresponding values.\n",
    "    \"\"\"\n",
    "    desired_tags = ['GPSLatitudeRef', 'GPSLatitude', 'GPSLongitudeRef', 'GPSLongitude', 'GPSAltitude', 'Model', 'FocalLength', \"DateTime\"]\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    exif_data = image._getexif()\n",
    "    exif_dict = {}\n",
    "\n",
    "    if exif_data is not None:\n",
    "        for key, value in exif_data.items():\n",
    "            if key in TAGS:\n",
    "                tag = TAGS[key]\n",
    "\n",
    "                if tag == 'GPSInfo':\n",
    "                    gps_data = {}\n",
    "                    for t in value:\n",
    "                        sub_tag = GPSTAGS.get(t, t)\n",
    "                        if sub_tag in desired_tags:\n",
    "                            gps_data[sub_tag] = value[t]\n",
    "                    exif_dict.update(gps_data)\n",
    "                elif tag in desired_tags:\n",
    "                    exif_dict[tag] = value\n",
    "\n",
    "    return exif_dict\n",
    "\n",
    "def get_geo_coord(lat, ref_lat, lon, ref_lon):\n",
    "    \"\"\"\n",
    "    Function to convert EXIF GPS coordinates to decimal format.\n",
    "    --------------------------------------------------------------\n",
    "    Parameters:\n",
    "    lat: A list of tuples containing the GPS latitude coordinates in degrees, minutes, and seconds.\n",
    "    ref_lat: A string representing the reference direction(N, S) of the GPS latitude coordinates.\n",
    "    lon: A list of tuples containing the GPS longitude coordinates in degrees, minutes, and seconds.\n",
    "    ref_lon: A string representing the reference direction(E, W) of the GPS longitude coordinates.\n",
    "    --------------------------------------------------------------\n",
    "    Returns:\n",
    "    A tuple of floats representing the GPS coordinates in decimal format.\n",
    "    \"\"\"\n",
    "    deg, minutes, seconds = lat\n",
    "    decimal_deg_lat = deg + (minutes / 60.0) + (seconds / 3600.0)\n",
    "\n",
    "    # Adjusting for the reference direction\n",
    "    if ref_lat == 'S':\n",
    "        decimal_deg_lat *= -1\n",
    "\n",
    "    deg, minutes, seconds = lon\n",
    "    decimal_deg_lon = deg + (minutes / 60.0) + (seconds / 3600.0)\n",
    "\n",
    "    # Adjusting for the reference direction\n",
    "    if ref_lon == 'W':\n",
    "        decimal_deg_lon *= -1\n",
    "\n",
    "    return decimal_deg_lat, decimal_deg_lon\n",
    "\n",
    "\n",
    "def calculate_fov_and_scene_dimensions(focal_length, sensor_width, sensor_height, distance):\n",
    "    # Calculate Field of View (FoV) in degrees\n",
    "    fov_horizontal = 2 * math.degrees(math.atan(sensor_width / (2 * focal_length)))\n",
    "    fov_vertical = 2 * math.degrees(math.atan(sensor_height / (2 * focal_length)))\n",
    "\n",
    "    # Calculate scene dimensions in kilometers\n",
    "    scene_width = 2 * distance * math.tan(math.radians(fov_horizontal) / 2)\n",
    "    scene_height = 2 * distance * math.tan(math.radians(fov_vertical) / 2)\n",
    "\n",
    "    return fov_horizontal, fov_vertical, scene_width, scene_height\n",
    "\n",
    "def mercator_projection(lat):\n",
    "    \"\"\"\n",
    "    Author: ChatGPT-4\n",
    "    This function converts latitude to Mercator projection.\n",
    "    --------------------------------------------------------------\n",
    "    Parameters:\n",
    "    lat (float): Latitude in degrees.\n",
    "    --------------------------------------------------------------\n",
    "    Returns:\n",
    "    float: Latitude in Mercator projection.\n",
    "    \"\"\"\n",
    "    return math.log(math.tan(math.radians(lat) / 2 + math.pi / 4))\n",
    "\n",
    "def calculate_mapbox_bounding_box(lat_center, lon_center, zoom, image_width, image_height):\n",
    "    \"\"\"\n",
    "    Author: ChatGPT-4\n",
    "    This function calculates the bounding box of a Mapbox map.\n",
    "    --------------------------------------------------------------\n",
    "    Parameters:\n",
    "    lat_center (float): Latitude of the center of the map.\n",
    "    lon_center (float): Longitude of the center of the map.\n",
    "    zoom (int): Zoom level of the map.\n",
    "    image_width (int): Width of the image.\n",
    "    image_height (int): Height of the image.\n",
    "    --------------------------------------------------------------\n",
    "    Returns:\n",
    "    tuple: A tuple containing the coordinates of the top-left, top-right, bottom-right, and bottom-left corners of the image.\n",
    "    \"\"\"\n",
    "    # Tile size (in pixels) used by Mapbox\n",
    "    tile_size = 512\n",
    "\n",
    "    # Number of tiles at the given zoom level\n",
    "    num_tiles = 2 ** zoom\n",
    "\n",
    "    # Scale factor at this zoom level\n",
    "    scale = num_tiles * tile_size\n",
    "\n",
    "    # Latitude in Mercator projection\n",
    "    mercator_lat = mercator_projection(lat_center)\n",
    "\n",
    "    # Convert center longitude and latitude to pixel values\n",
    "    pixel_x_center = (lon_center + 180) / 360 * scale\n",
    "    pixel_y_center = (1 - mercator_lat / math.pi) / 2 * scale\n",
    "\n",
    "    # Calculate pixel coordinates of the corners\n",
    "    pixel_x_left = pixel_x_center - image_width / 2\n",
    "    pixel_x_right = pixel_x_center + image_width / 2\n",
    "    pixel_y_top = pixel_y_center - image_height / 2\n",
    "    pixel_y_bottom = pixel_y_center + image_height / 2\n",
    "\n",
    "    # Convert pixel coordinates back to lat/lon\n",
    "    def pixels_to_latlon(px, py):\n",
    "        lon = px / scale * 360 - 180\n",
    "        lat = math.degrees(2 * math.atan(math.exp((1 - 2 * py / scale) * math.pi)) - math.pi / 2)\n",
    "        return lat, lon\n",
    "\n",
    "    top_left = pixels_to_latlon(pixel_x_left, pixel_y_top)\n",
    "    top_right = pixels_to_latlon(pixel_x_right, pixel_y_top)\n",
    "    bottom_right = pixels_to_latlon(pixel_x_right, pixel_y_bottom)\n",
    "    bottom_left = pixels_to_latlon(pixel_x_left, pixel_y_bottom)\n",
    "\n",
    "    return top_left, top_right, bottom_right, bottom_left\n",
    "\n",
    "\n",
    "def save_aoi_image(image, file_name, aoi_image_folder):\n",
    "    if image is not None:\n",
    "        # Remove the file extension and append '_aoi.jpg'\n",
    "        aoi_image_base_name = os.path.splitext(file_name)[0] + \"_aoi.jpg\"\n",
    "        \n",
    "        # Ensure the folder exists\n",
    "        os.makedirs(aoi_image_folder, exist_ok=True)\n",
    "        \n",
    "        # Combine the folder path with the modified file name\n",
    "        aoi_image_file_path = os.path.join(aoi_image_folder, aoi_image_base_name)\n",
    "\n",
    "        # Save the image in JPG format\n",
    "        image.save(aoi_image_file_path, \"JPEG\")\n",
    "        print(f\"AOI image saved at {aoi_image_file_path}\")\n",
    "    else:\n",
    "        print(\"No image to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google maps view extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# GoogleMapDownloader.py \n",
    "# Created by Adrien de Jaur√©guiberry\n",
    "#\n",
    "# A script which when given a longitude, latitude, length\n",
    "# returns a high resolution google map\n",
    "\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import os\n",
    "from math import *\n",
    "\n",
    "class GoogleMapDownloader:\n",
    "    \"\"\"\n",
    "        A class which generates high resolution google maps images given\n",
    "        a gmap API key and location parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, API_key, lat, lng, lgth, img_size=1000):\n",
    "        \"\"\"\n",
    "            GoogleMapDownloader Constructor\n",
    "            Args:\n",
    "                API_key:  The GoogleMap API key to load images\n",
    "                lat:      The latitude of the location required\n",
    "                lng:      The longitude of the location required\n",
    "                lgth:     Length of the map in m. The map will be a square.\n",
    "                          warning: too big length will result in distorded map due to mercator projection.\n",
    "                img_size: The resolution of the output image as img_size X img_size\n",
    "                          default to 1000\n",
    "        \"\"\"\n",
    "        lat_rad = (pi/180)*lat\n",
    "        self._img_size = img_size\n",
    "        self._lat = lat\n",
    "        self._lng = lng\n",
    "        self._zoom = floor(log2(156543.03 * img_size / lgth))\n",
    "        self._resolution = 156543.03 / (2 ** self._zoom) #(m/px)\n",
    "        self._nb_tiles = ceil(img_size/500)\n",
    "        self._tile_lgth = lgth/self._nb_tiles\n",
    "        self._tile_size = int(self._tile_lgth/self._resolution)\n",
    "        self._API_key = API_key\n",
    "\n",
    "    def getMercatorFromGPS(self,lng,lat):\n",
    "    \tx = 6371000 * lng\n",
    "    \ty = 6371000 * log(tan(pi/4 + lat/2))\n",
    "    \treturn (x,y)\n",
    "\n",
    "    def getGPSFromMercator(self,x,y):\n",
    "    \tlng = x/6371000\n",
    "    \tlat = 2*atan(exp(y/6371000)) - pi/2\n",
    "    \treturn (lng,lat)\n",
    "    \n",
    "    def get_zoom_level(self):\n",
    "        return self._zoom\n",
    "\n",
    "    def generateImage(self):\n",
    "        \"\"\"\n",
    "            Generates an image by stitching a number of google map tiles together.\n",
    "            \n",
    "            Returns:\n",
    "                A high-resolution Goole Map image.\n",
    "        \"\"\"\n",
    "\n",
    "        lat_rad = (pi/180)*abs(self._lat)\n",
    "        lng_rad = (pi/180)*abs(self._lng)\n",
    "        xy_loc  = self.getMercatorFromGPS(lng_rad,lat_rad)\n",
    "\n",
    "        xy_with_step  = [xy_loc[0]+self._tile_lgth , xy_loc[1]+self._tile_lgth]\n",
    "        gps_with_step = self.getGPSFromMercator(xy_with_step[0], xy_with_step[1])\n",
    "\n",
    "        lat_step = (180/pi)*(gps_with_step[1] - lat_rad)\n",
    "        lon_step = (180/pi)*(gps_with_step[0] - lng_rad)\n",
    "\n",
    "        border = 20        \n",
    "\n",
    "        # Determine the size of the image\n",
    "        width, height = self._tile_size * self._nb_tiles, self._tile_size * self._nb_tiles\n",
    "\n",
    "        #Create a new image of the size require\n",
    "        map_img = Image.new('RGB', (width,height))\n",
    "\n",
    "\n",
    "        nb_tiles_max = self._nb_tiles**2\n",
    "        counter = 1\n",
    "        for x in range(0, self._nb_tiles):\n",
    "            for y in range(0, self._nb_tiles) :\n",
    "\n",
    "                la = self._lat - y*lat_step + lat_step*(self._nb_tiles-1)/2\n",
    "                lo = self._lng + x*lon_step - lon_step*(self._nb_tiles-1)/2\n",
    "\n",
    "                url = 'https://maps.googleapis.com/maps/api/staticmap?'\n",
    "                url += 'center='+str(la)+','+str(lo)\n",
    "                url += '&zoom='+str(self._zoom)\n",
    "                url += '&size='+str(self._tile_size+2*border)+'x'+str(self._tile_size+2*border)\n",
    "                url += '&maptype=satellite'\n",
    "                if self._API_key:url += '&key='+self._API_key\n",
    "                print('getting tile '+str(counter)+\"/\"+str(nb_tiles_max))\n",
    "                counter+=1\n",
    "\n",
    "                current_tile = str(x)+'-'+str(y)\n",
    "                request.urlretrieve(url, current_tile)\n",
    "            \n",
    "                im = Image.open(current_tile)\n",
    "                map_img.paste(im.crop((border,border,self._tile_size+border,self._tile_size+border)), (x*self._tile_size, y*self._tile_size))\n",
    "              \n",
    "                os.remove(current_tile)\n",
    "\n",
    "        print(\"Resizing map\")\n",
    "        return map_img.resize((self._img_size,self._img_size))\n",
    "\n",
    "import os\n",
    "\n",
    "def download_map_image(api_key, lat, lng, map_length, image_size):\n",
    "    gmd = GoogleMapDownloader(api_key, lat, lng, map_length, image_size)\n",
    "    try:\n",
    "        # Generate the high-resolution map image\n",
    "        return gmd.generateImage()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not generate the image - {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get the zoom level\n",
    "def get_map_zoom(api_key, lat, lng, map_length, image_size):\n",
    "    gmd = GoogleMapDownloader(api_key, lat, lng, map_length, image_size)\n",
    "    return gmd.get_zoom_level()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Function to get image dimensions\n",
    "def get_image_dimensions(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found or the path is incorrect\")\n",
    "    return image.shape[1], image.shape[0]  # Width, Height\n",
    "\n",
    "def kernal_window_finder(image_width_px, image_height_px, covered_area_width_km, covered_area_height_km, target_area_width_km, target_area_height_km):\n",
    "    \"\"\"\n",
    "    Calculate the pixel dimensions for a given kilometer area in an image, rounded to the nearest whole number.\n",
    "\n",
    "    Parameters:\n",
    "    image_width_px (int): Width of the image in pixels.\n",
    "    image_height_px (int): Height of the image in pixels.\n",
    "    covered_area_width_km (float): Width of the area covered by the image in kilometers.\n",
    "    covered_area_height_km (float): Height of the area covered by the image in kilometers.\n",
    "    target_area_width_km (float): Width of the target area in kilometers.\n",
    "    target_area_height_km (float): Height of the target area in kilometers.\n",
    "\n",
    "    Returns:\n",
    "    (int, int): Width and height of the target area in pixels, rounded to the nearest whole number.\n",
    "    \"\"\"\n",
    "\n",
    "    # The number of pixels per km in both dimensions\n",
    "    pixels_per_km_width = image_width_px / covered_area_width_km\n",
    "    pixels_per_km_height = image_height_px / covered_area_height_km\n",
    "\n",
    "    # Calculating the pixel dimensions for the target area\n",
    "    target_area_width_px = round(target_area_width_km * pixels_per_km_width)\n",
    "    target_area_height_px = round(target_area_height_km * pixels_per_km_height)\n",
    "\n",
    "    return target_area_width_px, target_area_height_px\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def subsample_images(image_path, output_folder, window_width, window_height, stride_x, stride_y):\n",
    "    \"\"\"\n",
    "    Generate subsampled images from a reference image using a sliding window and print the total number of images generated.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): Path to the reference image.\n",
    "    output_folder (str): Path to the folder where subsampled images will be saved.\n",
    "    window_width (int): Width of the sliding window in pixels.\n",
    "    window_height (int): Height of the sliding window in pixels.\n",
    "    stride_x (int): Horizontal stride of the sliding window in pixels.\n",
    "    stride_y (int): Vertical stride of the sliding window in pixels.\n",
    "    \"\"\"\n",
    "    # Load the reference image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Count the number of subsamples generated\n",
    "    num_subsamples = 0\n",
    "\n",
    "    print(\"Generating subsampled images...\")\n",
    "\n",
    "    # Iterate over the image using the sliding window\n",
    "    for y in range(0, image.height - window_height + 1, stride_y):\n",
    "        for x in range(0, image.width - window_width + 1, stride_x):\n",
    "            # Extract the sub-image\n",
    "            sub_image = image.crop((x, y, x + window_width, y + window_height))\n",
    "\n",
    "            # Save the sub-image\n",
    "            sub_image_path = os.path.join(output_folder, f\"subsample_{x}_{y}.jpg\")\n",
    "            sub_image.save(sub_image_path)\n",
    "            num_subsamples += 1\n",
    "            # print(f\"Subsample saved: {sub_image_path}\")\n",
    "\n",
    "    # Print the total number of subsamples generated\n",
    "    print(f\"Total number of subsampled images generated: {num_subsamples}\")\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import time\n",
    "\n",
    "def resize_image(image, max_size=800):\n",
    "    h, w = image.shape[:2]\n",
    "    scale = max_size / max(h, w)\n",
    "\n",
    "    if scale < 1:\n",
    "        image = cv2.resize(image, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)\n",
    "    return image\n",
    "\n",
    "def sift_detect_and_compute(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    if descriptors is None:\n",
    "        return None, None\n",
    "    return keypoints, descriptors\n",
    "\n",
    "def match_features(descriptors1, descriptors2):\n",
    "    if descriptors1 is None or descriptors2 is None:\n",
    "        return []\n",
    "    matcher = cv2.BFMatcher()\n",
    "    matches = matcher.knnMatch(descriptors1, descriptors2, k=2)\n",
    "    # Ensure there are at least 2 matches to unpack\n",
    "    if len(matches) < 2:\n",
    "        return []\n",
    "    return matches\n",
    "\n",
    "\n",
    "def filter_good_matches(matches):\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.75 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    return good_matches\n",
    "\n",
    "def find_homography(keypoints1, keypoints2, good_matches):\n",
    "    if not good_matches:\n",
    "        return None, None\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    return M, mask\n",
    "\n",
    "def visualize_matches(image1, keypoints1, image2, keypoints2, matches, mask):\n",
    "    if mask is None:\n",
    "        print(\"Homography could not be computed.\")\n",
    "        return\n",
    "    draw_params = dict(matchColor=(0, 255, 0),\n",
    "                       singlePointColor=None,\n",
    "                       matchesMask=mask.ravel().tolist(),\n",
    "                       flags=2)\n",
    "    img_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches, None, **draw_params)\n",
    "    img_matches = cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(img_matches)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def match_query_to_scene(query_image_path, scene_image_path):\n",
    "    \n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    query_image = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    query_image = extract_features(query_image, model)\n",
    "    if query_image is None:\n",
    "        raise ValueError(f\"Could not load the query image from path: {query_image_path}\")\n",
    "    query_image = resize_image(query_image)\n",
    "\n",
    "    scene_image = cv2.imread(scene_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    scene_image = extract_features(scene_image, model)\n",
    "    \n",
    "    if scene_image is None:\n",
    "        raise ValueError(f\"Could not load the scene image from path: {scene_image_path}\")\n",
    "    scene_image = resize_image(scene_image)\n",
    "\n",
    "    keypoints_query, descriptors_query = sift_detect_and_compute(query_image)\n",
    "    keypoints_scene, descriptors_scene = sift_detect_and_compute(scene_image)\n",
    "\n",
    "    matches = match_features(descriptors_query, descriptors_scene)\n",
    "    good_matches = filter_good_matches(matches)\n",
    "\n",
    "    if len(good_matches) > 10:\n",
    "        M, mask = find_homography(keypoints_query, keypoints_scene, good_matches)\n",
    "        if M is not None:\n",
    "            matchesMask = mask.ravel().tolist()\n",
    "            matching_percentage = (sum(matchesMask) / len(good_matches)) * 100\n",
    "            print(f\"Match found with {scene_image_path}: {matching_percentage:.2f}%\")\n",
    "            return scene_image_path, matching_percentage, (keypoints_scene, good_matches, matchesMask)\n",
    "    else:\n",
    "        pass\n",
    "#         print(f\"Not enough good matches found for {scene_image_path}\")\n",
    "\n",
    "    return scene_image_path, 0, (None, None, None)  # Return zero if homography not found or not enough good matches\n",
    "\n",
    "# def calculate_center_pixel(keypoints_scene, good_matches, matchesMask):\n",
    "#     matched_pts = [keypoints_scene[m.trainIdx].pt for m, mask in zip(good_matches, matchesMask) if mask]\n",
    "#     if not matched_pts:\n",
    "#         return (0, 0)\n",
    "#     x, y = zip(*matched_pts)\n",
    "#     center_x, center_y = sum(x) / len(x), sum(y) / len(y)\n",
    "#     return int(center_x), int(center_y)\n",
    "\n",
    "# def pixel_to_geo(pixel_x, pixel_y, top_left_geo, bottom_right_geo, image_width_px, image_height_px):\n",
    "#     lat_range = bottom_right_geo[0] - top_left_geo[0]\n",
    "#     lon_range = bottom_right_geo[1] - top_left_geo[1]\n",
    "\n",
    "#     geo_x = top_left_geo[1] + (pixel_x / image_width_px) * lon_range\n",
    "#     geo_y = top_left_geo[0] + (pixel_y / image_height_px) * lat_range\n",
    "\n",
    "#     return geo_x, geo_y\n",
    "\n",
    "def calculate_pixel_to_geo(top_left_geo, bottom_right_geo, image_width_px, image_height_px, pixel_x, pixel_y):\n",
    "    # New function to calculate geographic coordinates from pixel coordinates\n",
    "    lat_range = bottom_right_geo[0] - top_left_geo[0]\n",
    "    lon_range = bottom_right_geo[1] - top_left_geo[1]\n",
    "\n",
    "    lat_per_pixel = lat_range / image_height_px\n",
    "    lon_per_pixel = lon_range / image_width_px\n",
    "\n",
    "    geo_x = top_left_geo[1] + pixel_x * lon_per_pixel\n",
    "    geo_y = top_left_geo[0] + pixel_y * lat_per_pixel\n",
    "\n",
    "    return geo_y, geo_x\n",
    "\n",
    "def compare_images_with_query(query_image_path, directory_path, num_matches=1):\n",
    "    best_match_paths = []\n",
    "\n",
    "    matches_info = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for image_path in glob.glob(directory_path + '/*.jpg'):\n",
    "        try:\n",
    "            image_path, matching_percentage, match_data = match_query_to_scene(query_image_path, image_path)\n",
    "            matches_info.append((image_path, matching_percentage, match_data))\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing images: {e}\")\n",
    "\n",
    "    matches_info.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get top N matches based on num_matches\n",
    "    for i in range(min(num_matches, len(matches_info))):\n",
    "        best_match_info = matches_info[i]\n",
    "        best_match_path, best_match_percentage, _ = best_match_info\n",
    "        print(f\"Visualizing match with {best_match_path}: {best_match_percentage:.2f}%\")\n",
    "        match_query_to_scene(query_image_path, best_match_path)\n",
    "        best_match_paths.append(best_match_path)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time taken: {total_time:.2f} seconds\")\n",
    "\n",
    "    return best_match_paths\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_subsample_offsets(best_match_path):\n",
    "    # Extract the file name without the directory\n",
    "    file_name = os.path.basename(best_match_path)\n",
    "    # Split the file name into parts\n",
    "    parts = file_name.split('_')\n",
    "    # The offsets are the last two parts before the file extension, convert them to integers\n",
    "    offset_x = int(parts[-2])\n",
    "    offset_y = int(parts[-1].split('.')[0])  # Remove the file extension before converting\n",
    "    return offset_x, offset_y\n",
    "\n",
    "def get_center_pixel_of_subsample(subsample_path):\n",
    "    subsample_image = cv2.imread(subsample_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if subsample_image is None:\n",
    "        raise ValueError(f\"Could not load the subsample image from path: {subsample_path}\")\n",
    "    h, w = subsample_image.shape[:2]\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    return center_x, center_y\n",
    "\n",
    "def calculate_subsample_geo_center(subsample_path, large_area_image_path, top_left_geo, bottom_right_geo, subsample_offset_x, subsample_offset_y):\n",
    "    # Get the dimensions of the large area image\n",
    "    large_image_width_px, large_image_height_px = get_image_dimensions(large_area_image_path)\n",
    "\n",
    "    # Calculate the center pixel of the subsample\n",
    "    center_pixel_x, center_pixel_y = get_center_pixel_of_subsample(subsample_path)\n",
    "\n",
    "    # Calculate the absolute pixel coordinates of the center in the large area image\n",
    "    absolute_center_x = subsample_offset_x + center_pixel_x\n",
    "    absolute_center_y = subsample_offset_y + center_pixel_y\n",
    "\n",
    "    # Convert these absolute pixel coordinates to geographic coordinates\n",
    "    geo_center = calculate_pixel_to_geo(top_left_geo, bottom_right_geo, large_image_width_px, large_image_height_px, absolute_center_x, absolute_center_y)\n",
    "    \n",
    "    return geo_center\n",
    "\n",
    "\n",
    "def extract_features(img, model):\n",
    "    \"\"\"\n",
    "    Function to preprocess the image and extract features\n",
    "    --------------------------------------------------------------\n",
    "    Parameters:\n",
    "    img: A PIL image object.\n",
    "    model: A Keras model object.\n",
    "    --------------------------------------------------------------\n",
    "    Returns:\n",
    "    A 4D NumPy array containing the extracted features.\n",
    "    \"\"\"\n",
    "    img_array = img_to_array(img.resize((512, 512)))\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    features = model.predict(img_array)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_iss_image(query_image_name, api_key):\n",
    "    # Loading query image and extracting metadata for Formentera\n",
    "\n",
    "    query_image_folder = \"../data/images/\"\n",
    "    query_image_name = query_image_name   \n",
    "    query_image_path = query_image_folder + query_image_name    \n",
    "\n",
    "    exif_data = get_exif_data(query_image_path)\n",
    "\n",
    "    print(f\"Extracted Metadata:\\n {exif_data}\")\n",
    "\n",
    "\n",
    "    gps_latitude = exif_data.get('GPSLatitude', None)\n",
    "    gps_latitude_ref = exif_data.get('GPSLatitudeRef', None)\n",
    "    gps_longitude = exif_data.get('GPSLongitude', None)\n",
    "    gps_longitude_ref = exif_data.get('GPSLongitudeRef', None)\n",
    "    gps_altitude = exif_data.get('GPSAltitude', None)\n",
    "    focal_length = exif_data.get('FocalLength', None)\n",
    "\n",
    "    # Check if all GPS data is available\n",
    "    if gps_latitude and gps_latitude_ref and gps_longitude and gps_longitude_ref:\n",
    "        lat, long = get_geo_coord(gps_latitude, gps_latitude_ref, gps_longitude, gps_longitude_ref)\n",
    "        print(f\"ISS coordinates: {lat}, {long}\")\n",
    "    else:\n",
    "        print(\"GPS data is incomplete or not available in the image.\")\n",
    "\n",
    "\n",
    "    # These correspond to a Nikon D5\n",
    "    sensor_width = 36  # in millimeters (for a full-frame sensor like Nikon D5)\n",
    "    sensor_height = 24  # in millimeters\n",
    "\n",
    "    # Distance of the ISS camera to earth\n",
    "    distance = gps_altitude /1000  # in kilometers\n",
    "\n",
    "    # scene dimensions in kilometers. It will help in finding the kernal size of sliding window.\n",
    "    fov_horizontal, fov_vertical, query_image_width_km, query_image_height_km = calculate_fov_and_scene_dimensions(focal_length, sensor_width, sensor_height, distance)\n",
    "    print(f\"scene width: {query_image_width_km} km and scene height: {query_image_height_km} km\")\n",
    "\n",
    "\n",
    "    # Downloading area of interest image and getting bounding box coordinates\n",
    "    api_key = api_key\n",
    "    map_length = 1200000  # in meters\n",
    "    image_size = 30000  # width and height in pixels\n",
    "    aoi_image_folder = \"../data/aoi_images/\"\n",
    "\n",
    "    # AOI image file path construction\n",
    "    aoi_image_base_name = os.path.splitext(query_image_name)[0] + \"_aoi.jpg\"\n",
    "    aoi_image_file_path = os.path.join(aoi_image_folder, aoi_image_base_name)\n",
    "\n",
    "    # Instantiate GoogleMapDownloader object\n",
    "    gmd = GoogleMapDownloader(api_key, lat, long, map_length, image_size)\n",
    "\n",
    "    # Check if AOI image already exists\n",
    "    if not os.path.exists(aoi_image_file_path):\n",
    "        # Download the map image if it doesn't exist\n",
    "        image = gmd.generateImage()\n",
    "\n",
    "        # Save the image if it is not None\n",
    "        if image is not None:\n",
    "            # Ensure the folder exists\n",
    "            os.makedirs(aoi_image_folder, exist_ok=True)\n",
    "\n",
    "            # Save the image in JPG format\n",
    "            image.save(aoi_image_file_path, \"JPEG\")\n",
    "            print(f\"AOI image saved at {aoi_image_file_path}\")\n",
    "        else:\n",
    "            print(\"No image to save.\")\n",
    "    else:\n",
    "        print(f\"AOI image already exists at {aoi_image_file_path}\")\n",
    "\n",
    "    # Retrieve the zoom level in both cases (new download or existing file)\n",
    "    zoom = gmd.get_zoom_level()\n",
    "    print(f\"Zoom level: {zoom}\")\n",
    "\n",
    "\n",
    "    # # manual check inputs:\n",
    "    # aoi_image_file_path =\"../data/aoi_images/iss050e070478_aoi.jpg\"\n",
    "    # zoom =11\n",
    "\n",
    "\n",
    "    # Calculate the bounding box coordinates\n",
    "    top_left_aoi, top_right_aoi, bottom_right_aoi, bottom_left_aoi = calculate_mapbox_bounding_box(lat, long, zoom, image_size, image_size)\n",
    "    print(\"top_left_aoi:\",top_left_aoi)\n",
    "    print(\"top_right_aoi:\",top_right_aoi)\n",
    "    print(\"bottom_right_aoi:\",bottom_right_aoi)\n",
    "    print(\"bottom_left_aoi:\",bottom_left_aoi)\n",
    "\n",
    "\n",
    "    # Using SIFT matcher\n",
    "    distance_km = map_length/2000  # convert meters in kilometers\n",
    "    rounded_width_km = math.ceil(query_image_width_km)\n",
    "    rounded_height_km = math.ceil(query_image_height_km)\n",
    "    kernal_width_px, kernal_height_px = kernal_window_finder(image_size, image_size, distance_km, distance_km, rounded_width_km, rounded_height_km)\n",
    "    print(\"Kernal Width in pixels:\", kernal_width_px, \", Kernal Height in pixels:\", kernal_height_px)\n",
    "\n",
    "    window_width = math.ceil(kernal_width_px*1.5)\n",
    "    window_height = math.ceil(kernal_height_px*1.5)\n",
    "    stride_x = kernal_width_px  # You can adjust the stride as needed\n",
    "    stride_y = kernal_height_px  # You can adjust the stride as needed\n",
    "\n",
    "    # Extract the base name without extension\n",
    "    base_name = os.path.splitext(query_image_name)[0]\n",
    "    # Create output folder path\n",
    "    aoi_subsample_output_folder_base = \"../data/aoi_subsampled_images/\"\n",
    "    aoi_subsample_output_folder_path = os.path.join(aoi_subsample_output_folder_base, \"subsampled_\" + base_name)\n",
    "    # Check if the folder exists, if not, create it\n",
    "    if not os.path.exists(aoi_subsample_output_folder_path):\n",
    "        os.makedirs(aoi_subsample_output_folder_path)\n",
    "        # Call the function \n",
    "        subsample_images(aoi_image_file_path, aoi_subsample_output_folder_path, window_width, window_height, stride_x, stride_y)\n",
    "    else:\n",
    "        print(f\"Subsampled images already exist at {aoi_subsample_output_folder_path}\")\n",
    "\n",
    "    # Run the comparison and get top N matches\n",
    "    num_matches = 2  # Set the number of matches you want\n",
    "    best_match_paths = compare_images_with_query(query_image_path, aoi_subsample_output_folder_path, num_matches)\n",
    "    large_area_image_path = aoi_image_file_path \n",
    "    top_left_geo = top_left_aoi\n",
    "    bottom_right_geo = bottom_right_aoi\n",
    "\n",
    "    # Initialize a list to store geographic centers\n",
    "    geo_centers = []\n",
    "\n",
    "    # Process each match in the list\n",
    "    for match_path in best_match_paths:\n",
    "        # Extract the subsample offsets\n",
    "        subsample_offset_x, subsample_offset_y = extract_subsample_offsets(match_path)\n",
    "\n",
    "        # Calculate the geographic center coordinates\n",
    "        geo_center = calculate_subsample_geo_center(match_path, large_area_image_path, top_left_geo, bottom_right_geo, subsample_offset_x, subsample_offset_y)\n",
    "        print(f\"Geographic center of {match_path}: {geo_center}\")\n",
    "\n",
    "        # Append the geographic center to the list\n",
    "        geo_centers.append(geo_center)\n",
    "    \n",
    "    return geo_centers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and writing the result excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: iss050e070478\n",
      "Extracted Metadata:\n",
      " {'GPSLatitudeRef': 'N', 'GPSLatitude': (42.0, 10.3992, 0.0), 'GPSLongitudeRef': 'W', 'GPSLongitude': (74.0, 36.3344, 0.0), 'GPSAltitude': 404564.54545454547, 'Model': 'NIKON D4', 'DateTime': '2017:10:16 09:38:48', 'FocalLength': 1150.0}\n",
      "ISS coordinates: 42.17332, -74.60557333333334\n",
      "scene width: 12.664629249011858 km and scene height: 8.443086166007905 km\n",
      "AOI image already exists at ../data/aoi_images/iss050e070478_aoi.jpg\n",
      "Zoom level: 11\n",
      "top_left_aoi: (45.874466212051956, -79.75541464192709)\n",
      "top_right_aoi: (45.874466212051956, -69.45573202473959)\n",
      "bottom_right_aoi: (38.242217020514246, -69.45573202473959)\n",
      "bottom_left_aoi: (38.242217020514246, -79.75541464192709)\n",
      "Kernal Width in pixels: 650 , Kernal Height in pixels: 450\n",
      "Subsampled images already exist at ../data/aoi_subsampled_images/subsampled_iss050e070478\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n",
      "Error comparing images: cannot resize an array that references or is referenced\n",
      "by another array in this way.\n",
      "Use the np.resize function or refcheck=False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\BOSTON UNIVERSITY\\CAS CS 549\\ml-terc-image-geolocation\\notebooks\\sliding_window_matcher_final_NN+SIFT.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m query_image_name \u001b[39m=\u001b[39m image_cell\u001b[39m.\u001b[39mvalue \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Call your function to get the geographic centers\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m geo_centers \u001b[39m=\u001b[39m process_iss_image(query_image_name, api_key)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Update the cell with the list of geo centers\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m location_from_code_cell\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00mlat\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mlon\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m lat, lon \u001b[39min\u001b[39;00m geo_centers)\n",
      "\u001b[1;32me:\\BOSTON UNIVERSITY\\CAS CS 549\\ml-terc-image-geolocation\\notebooks\\sliding_window_matcher_final_NN+SIFT.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m# Run the comparison and get top N matches\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m num_matches \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m  \u001b[39m# Set the number of matches you want\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m best_match_paths \u001b[39m=\u001b[39m compare_images_with_query(query_image_path, aoi_subsample_output_folder_path, num_matches)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m large_area_image_path \u001b[39m=\u001b[39m aoi_image_file_path \n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m top_left_geo \u001b[39m=\u001b[39m top_left_aoi\n",
      "\u001b[1;32me:\\BOSTON UNIVERSITY\\CAS CS 549\\ml-terc-image-geolocation\\notebooks\\sliding_window_matcher_final_NN+SIFT.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_path \u001b[39min\u001b[39;00m glob\u001b[39m.\u001b[39mglob(directory_path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/*.jpg\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m         image_path, matching_percentage, match_data \u001b[39m=\u001b[39m match_query_to_scene(query_image_path, image_path)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m         matches_info\u001b[39m.\u001b[39mappend((image_path, matching_percentage, match_data))\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=219'>220</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32me:\\BOSTON UNIVERSITY\\CAS CS 549\\ml-terc-image-geolocation\\notebooks\\sliding_window_matcher_final_NN+SIFT.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch_query_to_scene\u001b[39m(query_image_path, scene_image_path):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=145'>146</a>\u001b[0m     model \u001b[39m=\u001b[39m VGG16(weights\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimagenet\u001b[39m\u001b[39m'\u001b[39m, include_top\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m     query_image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(query_image_path, cv2\u001b[39m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/BOSTON%20UNIVERSITY/CAS%20CS%20549/ml-terc-image-geolocation/notebooks/sliding_window_matcher_final_NN%2BSIFT.ipynb#X14sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     query_image \u001b[39m=\u001b[39m extract_features(query_image, model)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\applications\\vgg16.py:199\u001b[0m, in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m# Block 5\u001b[39;00m\n\u001b[0;32m    196\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\n\u001b[0;32m    197\u001b[0m     \u001b[39m512\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock5_conv1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m )(x)\n\u001b[1;32m--> 199\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\n\u001b[0;32m    200\u001b[0m     \u001b[39m512\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock5_conv2\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    201\u001b[0m )(x)\n\u001b[0;32m    202\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mConv2D(\n\u001b[0;32m    203\u001b[0m     \u001b[39m512\u001b[39m, (\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m), activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msame\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock5_conv3\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m )(x)\n\u001b[0;32m    205\u001b[0m x \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mMaxPooling2D((\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), strides\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mblock5_pool\u001b[39m\u001b[39m\"\u001b[39m)(x)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:1063\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1055\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[39m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[0;32m   1061\u001b[0m     \u001b[39mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[0;32m   1062\u001b[0m ):\n\u001b[1;32m-> 1063\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_functional_construction_call(\n\u001b[0;32m   1064\u001b[0m         inputs, args, kwargs, input_list\n\u001b[0;32m   1065\u001b[0m     )\n\u001b[0;32m   1067\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2593\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   2587\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   2588\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value\n\u001b[0;32m   2589\u001b[0m ):\n\u001b[0;32m   2590\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[0;32m   2591\u001b[0m     \u001b[39m# shape.\u001b[39;00m\n\u001b[0;32m   2592\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2593\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keras_tensor_symbolic_call(\n\u001b[0;32m   2594\u001b[0m             inputs, input_masks, args, kwargs\n\u001b[0;32m   2595\u001b[0m         )\n\u001b[0;32m   2596\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   2597\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mDictWrapper\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2439\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m   2435\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   2436\u001b[0m         keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature\n\u001b[0;32m   2437\u001b[0m     )\n\u001b[0;32m   2438\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_infer_output_signature(\n\u001b[0;32m   2440\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[0;32m   2441\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\engine\\base_layer.py:2498\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m   2496\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   2497\u001b[0m         inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m-> 2498\u001b[0m         outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2500\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m   2501\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_mask_metadata(\n\u001b[0;32m   2502\u001b[0m     inputs, outputs, input_masks, build_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2503\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[0;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[1;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[0;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:321\u001b[0m, in \u001b[0;36mConv.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    318\u001b[0m     outputs\u001b[39m.\u001b[39mset_shape(out_shape)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(outputs)\n\u001b[0;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\activations.py:306\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.activations.relu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mdispatch\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m    270\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(x, alpha\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, max_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, threshold\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m):\n\u001b[0;32m    271\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Applies the rectified linear unit activation function.\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[0;32m    273\u001b[0m \u001b[39m    With default values, this returns the standard ReLU activation:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39m        input `x`.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39mrelu(\n\u001b[0;32m    307\u001b[0m         x, alpha\u001b[39m=\u001b[39malpha, max_value\u001b[39m=\u001b[39mmax_value, threshold\u001b[39m=\u001b[39mthreshold\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\keras\\src\\backend.py:5397\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(x, alpha, max_value, threshold)\u001b[0m\n\u001b[0;32m   5395\u001b[0m     clip_max \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   5396\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 5397\u001b[0m     x \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m   5399\u001b[0m \u001b[39mif\u001b[39;00m clip_max:\n\u001b[0;32m   5400\u001b[0m     max_value \u001b[39m=\u001b[39m _constant_to_tensor(max_value, x\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:12296\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(features, name)\u001b[0m\n\u001b[0;32m  12294\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m  12295\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m> 12296\u001b[0m   _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39m_apply_op_helper(\n\u001b[0;32m  12297\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mRelu\u001b[39m\u001b[39m\"\u001b[39m, features\u001b[39m=\u001b[39mfeatures, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m  12298\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m  12299\u001b[0m   _result \u001b[39m=\u001b[39m _dispatch\u001b[39m.\u001b[39mdispatch(\n\u001b[0;32m  12300\u001b[0m         relu, (), \u001b[39mdict\u001b[39m(features\u001b[39m=\u001b[39mfeatures, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m  12301\u001b[0m       )\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    792\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    793\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    794\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    795\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    797\u001b[0m                              name\u001b[39m=\u001b[39mscope, input_types\u001b[39m=\u001b[39minput_types,\n\u001b[0;32m    798\u001b[0m                              attrs\u001b[39m=\u001b[39mattr_protos, op_def\u001b[39m=\u001b[39mop_def)\n\u001b[0;32m    800\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2657\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2655\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2657\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39mfrom_node_def(\n\u001b[0;32m   2658\u001b[0m       node_def,\n\u001b[0;32m   2659\u001b[0m       \u001b[39mself\u001b[39m,\n\u001b[0;32m   2660\u001b[0m       inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m   2661\u001b[0m       output_types\u001b[39m=\u001b[39mdtypes,\n\u001b[0;32m   2662\u001b[0m       control_inputs\u001b[39m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   2663\u001b[0m       input_types\u001b[39m=\u001b[39minput_types,\n\u001b[0;32m   2664\u001b[0m       original_op\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_original_op,\n\u001b[0;32m   2665\u001b[0m       op_def\u001b[39m=\u001b[39mop_def,\n\u001b[0;32m   2666\u001b[0m   )\n\u001b[0;32m   2667\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   2668\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1161\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1158\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   1160\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1161\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39mop_def)\n\u001b[0;32m   1162\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\akalps\\anaconda2023\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1018\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1014\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n\u001b[0;32m   1015\u001b[0m                                          serialized)\n\u001b[0;32m   1017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1018\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1019\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1020\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from openpyxl import load_workbook\n",
    "import os\n",
    "\n",
    "# Replace with the path to your Excel file\n",
    "excel_file_path = '../results/results_NN+SIFT.xlsx'\n",
    "api_key = \"AIzaSyCxRtWpRRPYvcPlQl440mjoXWrvModq4XQ\"\n",
    "\n",
    "# Load the workbook and select the active worksheet\n",
    "wb = load_workbook(excel_file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Iterate through the rows in the worksheet\n",
    "for row in ws.iter_rows(min_row=2, max_col=5):\n",
    "    image_cell, location_from_code_cell = row[0], row[4]\n",
    "\n",
    "    # If 'Location from code' is empty and 'Image' is not empty\n",
    "    if not location_from_code_cell.value and image_cell.value:\n",
    "        print(f\"Processing image: {image_cell.value}\")\n",
    "        query_image_name = image_cell.value + \".jpg\"\n",
    "\n",
    "        # Call your function to get the geographic centers\n",
    "        geo_centers = process_iss_image(query_image_name, api_key)\n",
    "\n",
    "        # Update the cell with the list of geo centers\n",
    "        location_from_code_cell.value = ', '.join(f\"({lat}, {lon})\" for lat, lon in geo_centers)\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(excel_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > SIFT_matcher_requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA915f0wWqoCTWRdp9vlsI",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
